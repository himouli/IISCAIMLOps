Long Quiz: GPT Models & Large Language Models (LLMs) - Advanced Practice Set

Part A [10 x 3 points = 30] : Multiple Choice Questions (MCQs)

What architectural modification enables GPT-4 to surpass GPT-3.5 in multimodal understanding?
A. Increased embedding dimensions
B. Hybrid integration of text and vision encoders
C. Reduced parameter scaling
D. Complete removal of self-attention layers

Answer: B

Which optimization technique is essential for scaling LLMs beyond a trillion parameters?
A. Gradient clipping
B. Mixed-precision training
C. Batch normalization
D. Hard attention pruning

Answer: B

What key limitation does Retrieval-Augmented Generation (RAG) address in LLMs?
A. Overfitting on training data
B. Lack of real-time knowledge integration
C. Bias elimination
D. Reduced memory consumption

Answer: B

Why does GPT training heavily rely on unsupervised learning paradigms?
A. To maximize data labeling efficiency
B. To enhance generalization over unseen tasks
C. To optimize inference latency
D. To replace all reinforcement learning components

Answer: B

How does gradient checkpointing contribute to efficient LLM training?
A. Reduces memory usage by recomputing activations on demand
B. Eliminates the need for dropout layers
C. Speeds up backpropagation by caching gradients
D. Enhances positional embedding accuracy

Answer: A

What is a primary risk of relying solely on prompt engineering for model alignment?
A. Increased token redundancy
B. Potential adversarial exploits and biases
C. Slower fine-tuning processes
D. Reduced dataset scalability

Answer: B

Which factor contributes to improved factual consistency in large-scale generative models?
A. Increased model width and depth
B. Use of retrieval-based augmentation
C. Higher tokenization granularity
D. Reduced sequence length

Answer: B

What challenge does LoRA address in fine-tuning massive LLMs?
A. Reducing the number of active model layers
B. Enabling low-rank weight adaptation without full parameter updates
C. Improving sentence boundary detection
D. Increasing token generation speed

Answer: B

How does speculative decoding improve autoregressive LLM inference?
A. By enabling parallel hypothesis evaluation
B. By increasing the attention span of the model
C. By pre-training on token pairs
D. By using reinforcement learning to reject incorrect generations

Answer: A

Why are adapter layers beneficial in parameter-efficient fine-tuning?
A. They store compressed representations of activations
B. They allow selective weight adaptation without full backpropagation
C. They increase dropout effectiveness
D. They replace transformer blocks entirely

Answer: B

Part B [4 x 2 points = 8] : Multiple Choice Questions (MCQs)

What is a fundamental tradeoff when using larger LLMs?
A. Higher generalization but lower specificity
B. Increased speed but reduced expressiveness
C. Lower parameter count but better accuracy
D. Reduced inference costs but higher data bias

Answer: A

Why does token-level reinforcement learning pose challenges in LLM fine-tuning?
A. It struggles with delayed rewards due to sequence generation dependencies
B. It completely eliminates zero-shot capabilities
C. It requires explicit grammar supervision
D. It lacks convergence in pre-trained models

Answer: A

Which factor primarily determines the efficiency of attention mechanisms in LLMs?
A. Softmax temperature adjustments
B. Attention head diversity and sparsity patterns
C. Increased positional encoding resolution
D. Elimination of layer normalization

Answer: B

How does QLoRA optimize memory efficiency in LLM fine-tuning?
A. By integrating quantized low-rank updates
B. By reducing attention layers
C. By caching activations for longer context retention
D. By modifying positional encoding schemes

Answer: A

Part C [2 x 6 points = 12]: MCQs with Explanation

Why is knowledge distillation critical in training smaller versions of LLMs?
A. It transfers knowledge from larger models while retaining essential task performance
B. It increases training speed by using smaller datasets
C. It optimizes memory footprint without affecting accuracy
D. It allows models to bypass training altogether

Answer: A
Explanation: Knowledge distillation helps smaller models mimic the behavior of larger counterparts while maintaining essential learned patterns.

What is the core advantage of mixture of experts (MoE) architectures in LLMs?
A. Selective activation of parameter subsets for efficiency
B. Removal of cross-attention modules
C. Use of only static embeddings for pre-training
D. Elimination of dropout layers

Answer: A
Explanation: MoE selectively activates a subset of expert networks per input, optimizing computational efficiency while maintaining performance.

This completes an exclusive 25-question high-complexity quiz for advanced exam practice.

