Long Quiz: GPT Models & Large Language Models (LLMs)

Part A [10 x 3 points = 30] : Multiple Choice Questions (MCQs)

What distinguishes GPT-class models from traditional language models?
A. Use of recurrent layers
B. Transformer-based architecture
C. Dependence on explicit syntactic rules
D. Manual feature engineering

Answer: B

Which type of data is required for training a GPT model?
A. Labeled datasets only
B. Large-scale unstructured text data
C. Only tokenized word lists
D. Numerical tabular data

Answer: B

What is the main objective of fine-tuning a GPT model?
A. To train a model from scratch
B. To adapt a pre-trained model to specific tasks
C. To replace the original model weights
D. To perform hyperparameter tuning

Answer: B

Which of the following is an open-source LLM?
A. GPT-4
B. Falcon
C. Gemini
D. Claude

Answer: B

What is a primary risk associated with LLMs?
A. Limited scalability
B. Hallucination and misinformation
C. Inability to process large datasets
D. Slow token generation speed

Answer: B

What does Zero-Shot Learning (ZSL) in LLMs mean?
A. The model is trained on zero data
B. The model can handle tasks without explicit training examples
C. The model generates zero probability outputs
D. The model requires additional fine-tuning

Answer: B

Which prompting strategy involves breaking down a problem into multiple logical steps?
A. Zero-Shot Learning (ZSL)
B. Few-Shot Learning (FSL)
C. Chain of Thought (CoT)
D. Standard prompting

Answer: C

What is the key advantage of ReAct prompting strategy?
A. It removes the need for embeddings
B. It combines reasoning with interactive actions
C. It improves inference speed
D. It eliminates bias completely

Answer: B

What does LoRA (Low-Rank Adaptation) aim to achieve?
A. Full model retraining
B. Efficient parameter fine-tuning
C. Tokenization improvement
D. Reducing hallucinations

Answer: B

How does QLoRA improve LoRA?
A. Uses fewer trainable parameters
B. Utilizes quantized model weights for memory efficiency
C. Increases model size for better performance
D. Applies zero-shot learning techniques

Answer: B

Part B [4 x 2 points = 8] : Multiple Choice Questions (MCQs)

What is the primary difference between GPT-3 and GPT-4?
A. GPT-4 has fewer parameters
B. GPT-4 improves multimodal capabilities
C. GPT-4 uses RNNs instead of Transformers
D. GPT-4 is less accurate than GPT-3

Answer: B

Which factor significantly affects the bias in LLMs?
A. Model size only
B. Training data distribution
C. Number of model layers
D. Tokenization strategy

Answer: B

What is a challenge when using LLMs for real-world applications?
A. Too much explainability
B. High computational costs
C. Limited vocabulary
D. Fixed knowledge base

Answer: B

What is one of the key components of taxonomy of interaction with LLMs?
A. Hardcoded responses
B. Structured and unstructured prompting
C. Manual keyword extraction
D. Syntax tree analysis

Answer: B

Part C [2 x 6 points = 12]: MCQs with Explanation

Why is parameter-efficient fine-tuning (PEFT) crucial for adapting LLMs?
A. It enables training from scratch more effectively
B. It allows models to be fine-tuned using fewer resources
C. It requires re-training the entire network
D. It removes the need for backpropagation

Answer: B
Explanation: PEFT methods like LoRA and QLoRA modify only a small set of parameters, reducing computational cost while adapting large models efficiently.

How does Few-Shot Learning (FSL) improve model adaptability?
A. By requiring large amounts of labeled data
B. By using small examples to generalize across tasks
C. By restricting the number of model layers
D. By relying solely on reinforcement learning

Answer: B
Explanation: FSL enables LLMs to understand new tasks with minimal examples, enhancing their generalization capabilities without extensive retraining.

This completes the 25-question long quiz based on GPT models and LLM interaction strategies.

