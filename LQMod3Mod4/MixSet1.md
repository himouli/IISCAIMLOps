ong Quiz: Advanced NLP & Transformer Models - High Complexity Practice Set

Part A [10 x 3 points = 30] : Multiple Choice Questions (MCQs)

What is the primary challenge in training BERT with dynamic masking?
A. Increased memory consumption
B. Mismatched pretraining and fine-tuning objectives
C. Reduced ability to generalize across languages
D. Lack of scalability in the transformer architecture

Answer: B

Why do transformer models struggle with long sequence dependencies?
A. Quadratic complexity of self-attention
B. Limited vocabulary size
C. Need for recurrent layers in deep networks
D. Excessive reliance on tokenization

Answer: A

What role does the feed-forward network (FFN) play in a transformer block?
A. Enhances token embeddings via convolution
B. Captures hierarchical syntactic structures
C. Adds non-linearity and improves feature transformation
D. Controls the attention mechanism directly

Answer: C

How does the Reversible Transformer differ from standard transformers?
A. It uses memory-efficient reversible layers
B. It eliminates the need for self-attention
C. It replaces all matrix multiplications with CNNs
D. It exclusively applies reinforcement learning

Answer: A

What is the impact of increasing the number of attention heads in a transformer?
A. Leads to greater context-awareness but increases computational cost
B. Decreases training efficiency and accuracy
C. Reduces the number of trainable parameters
D. Eliminates the need for positional encodings

Answer: A

Why is sinusoidal positional encoding beneficial in transformer architectures?
A. It generalizes better to unseen sequence lengths
B. It replaces self-attention mechanisms
C. It decreases the number of trainable parameters
D. It avoids overfitting in deep networks

Answer: A

What distinguishes ALiBi (Attention with Linear Biases) from traditional positional encodings?
A. It provides context-dependent attention scaling without explicit embeddings
B. It eliminates token-level dependencies
C. It replaces self-attention with recurrent layers
D. It requires additional fine-tuning steps

Answer: A

Why is self-supervised learning crucial in modern NLP?
A. Enables learning from large amounts of unlabeled text
B. Eliminates the need for tokenization
C. Prevents models from overfitting
D. Increases dataset bias

Answer: A

What does contrastive learning contribute to NLP models like SimCSE?
A. Improves sentence representation by maximizing similarity between augmented views
B. Eliminates the need for pretraining
C. Reduces model size while maintaining accuracy
D. Increases inference speed in zero-shot applications

Answer: A

How does the T5 (Text-to-Text Transfer Transformer) model differ from BERT?
A. Uses an encoder-decoder structure instead of a bidirectional transformer
B. Eliminates the need for attention heads
C. Focuses exclusively on masked language modeling
D. Uses CNN-based encoders for tokenization

Answer: A

Part B [4 x 2 points = 8] : Multiple Choice Questions (MCQs)

What is the primary challenge in applying Transformers to speech processing?
A. High variance in speech signal lengths
B. Lack of suitable pretraining objectives
C. Difficulty in tokenizing continuous signals
D. Poor generalization to downstream tasks

Answer: C

What optimization technique does Adafactor introduce to improve transformer training?
A. Reduced memory footprint via adaptive learning rate scaling
B. Enhanced attention span with extended sequence lengths
C. Use of dense layers to replace self-attention
D. Zero initialization for all parameters

Answer: A

How does GShard improve the efficiency of training massive NLP models?
A. By sharding model layers across distributed hardware
B. By using reinforcement learning-based optimization
C. By reducing the number of parameters in self-attention
D. By eliminating the need for dataset pre-processing

Answer: A

Why do masked language models like BERT use bidirectional training?
A. To enable contextual understanding from both past and future tokens
B. To speed up inference time in real-world applications
C. To eliminate the need for attention heads
D. To replace fine-tuning with self-training

Answer: A

Part C [2 x 6 points = 12]: MCQs with Explanation

What makes mixture-of-experts (MoE) models computationally efficient?
A. They selectively activate only a subset of parameters per input
B. They replace attention layers with recurrent architectures
C. They reduce the number of training steps required
D. They generalize well without requiring fine-tuning

Answer: A
Explanation: MoE architectures distribute computation across multiple expert layers, activating only necessary pathways, thus optimizing training efficiency.

How does Reinforcement Learning from Human Feedback (RLHF) improve NLP model alignment?
A. It fine-tunes models based on human preferences to avoid harmful outputs
B. It reduces tokenization errors in long sequences
C. It increases the frequency of rare word occurrences
D. It enhances zero-shot learning capabilities

Answer: A
Explanation: RLHF helps align generative models with human values, reducing biases and preventing unsafe model behavior.

This completes an exclusive 25-question high-complexity quiz on advanced NLP and transformer topics for exam practice.

