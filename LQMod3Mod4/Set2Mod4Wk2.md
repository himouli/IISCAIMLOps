Long Quiz: Advanced NLP Techniques & LLMs - Practice Set

Part A [10 x 3 points = 30] : Multiple Choice Questions (MCQs)

What is a key challenge of using Zero-Shot Learning (ZSL) in real-world applications?
A. Requires large labeled datasets
B. Struggles with domain-specific tasks
C. Always outperforms fine-tuned models
D. Reduces training time significantly

Answer: B

How does Few-Shot Learning (FSL) improve over traditional fine-tuning?
A. Uses fewer labeled examples for adaptation
B. Requires full dataset retraining
C. Replaces embeddings with rule-based models
D. Works only on structured data

Answer: A

Which prompting method integrates interactive steps with logical reasoning?
A. Chain of Thought (CoT)
B. Few-Shot Learning (FSL)
C. Zero-Shot Learning (ZSL)
D. Tokenization Optimization

Answer: A

How does ReAct differ from standard prompting strategies?
A. Integrates external environment interactions
B. Reduces computational complexity
C. Eliminates the need for embeddings
D. Uses rule-based approaches

Answer: A

What is the primary goal of Direct Signal Processing (DSP) in NLP?
A. Enhancing the efficiency of embedding layers
B. Converting unstructured text into structured data
C. Optimizing text tokenization for low-latency applications
D. Reducing overfitting in deep neural networks

Answer: A

What makes instruction tuning essential in large-scale LLMs?
A. Aligns models with human-readable task descriptions
B. Eliminates the need for transformer layers
C. Improves memory efficiency without retraining
D. Increases token redundancy

Answer: A

How does orchestration enhance LLM pipeline efficiency?
A. Coordinates multiple components to streamline workflows
B. Reduces training data requirements
C. Limits the number of function calls
D. Eliminates the need for pretraining

Answer: A

What is a primary advantage of Retrieval-Augmented Generation (RAG)?
A. Reduces reliance on static training data
B. Increases the number of required parameters
C. Improves model explainability but decreases efficiency
D. Eliminates the need for reinforcement learning

Answer: A

How do LLM Guardrails contribute to model safety?
A. Implement policies to prevent harmful outputs
B. Enhance tokenization accuracy
C. Increase model inference speed
D. Reduce dataset augmentation needs

Answer: A

What differentiates LLM Agents from traditional chatbot models?
A. Use of goal-driven decision-making
B. Reliance solely on predefined responses
C. Require no external context for reasoning
D. Are optimized only for structured data

Answer: A

Part B [4 x 2 points = 8] : Multiple Choice Questions (MCQs)

Why is parameter-efficient fine-tuning (PEFT) crucial for large LLMs?
A. Reduces resource costs while maintaining performance
B. Increases the model’s parameter count
C. Eliminates the need for instruction tuning
D. Requires full dataset retraining

Answer: A

What is a drawback of traditional RAG implementations?
A. Computational overhead from document retrieval
B. Reduces model accuracy over time
C. Eliminates the need for prompt engineering
D. Limits generalization across tasks

Answer: A

What additional safety mechanism can prevent LLM misuse?
A. Real-time adversarial filtering
B. Increasing model complexity
C. Removing self-attention layers
D. Reducing dataset size

Answer: A

How does an LLM agent enhance workflow automation?
A. Integrates reasoning and task execution
B. Reduces the need for external API calls
C. Eliminates model explainability concerns
D. Increases reliance on structured data

Answer: A

Part C [2 x 6 points = 12]: MCQs with Explanation

What challenge does Chain of Thought (CoT) prompting solve in NLP?
A. Improves reasoning for complex problem-solving tasks
B. Reduces hallucination issues in generative models
C. Eliminates dataset size dependency
D. Reduces response time in low-latency applications

Answer: A
Explanation: CoT prompting enhances LLMs’ ability to perform multi-step reasoning by breaking problems into structured logical steps.

Why is Reinforcement Learning from Human Feedback (RLHF) important for aligning LLMs?
A. Helps models generate responses that align with human values
B. Eliminates bias in training data completely
C. Enhances tokenization strategies
D. Optimizes GPU memory usage

Answer: A
Explanation: RLHF fine-tunes models using human preference feedback to ensure ethical and beneficial AI responses.
