LMs) - Practice Set

Part A [10 x 3 points = 30] : Multiple Choice Questions (MCQs)

What is the primary reason GPT models are considered autoregressive?
A. They process input bidirectionally
B. They generate text token by token based on previous outputs
C. They require labeled data for training
D. They use explicit grammar rules

Answer: B

What is a common issue faced when training GPT models?
A. Lack of hardware acceleration
B. Mode collapse
C. Overfitting to large-scale data
D. Frequent updates to architecture

Answer: C

Which component of GPT is responsible for learning long-range dependencies?
A. Fully connected layers
B. Transformer decoder layers
C. Embedding layers
D. Convolutional layers

Answer: B

What is a primary concern with deploying commercial LLMs?
A. High interpretability
B. Lack of computational cost
C. Ethical risks such as misinformation
D. Limited multilingual capabilities

Answer: C

What is one major limitation of fine-tuning GPT models?
A. Requires minimal computational resources
B. Can lead to catastrophic forgetting
C. Enhances generalization across domains
D. Does not require training data

Answer: B

How does tokenization impact LLM efficiency?
A. Reduces the number of computations required
B. Eliminates the need for attention mechanisms
C. Requires handcrafted rules for every language
D. Slows down training significantly

Answer: A

What is a major benefit of using open-source LLMs?
A. Fully controlled data privacy
B. Requires exclusive access through APIs
C. Limited training adaptability
D. Minimal community support

Answer: A

What does instruction tuning aim to achieve in LLMs?
A. Improve understanding of user queries through task-specific prompts
B. Increase the number of parameters
C. Improve performance only on sentiment analysis
D. Replace traditional model training

Answer: A

Why is quantization applied to large-scale LLMs?
A. To increase model accuracy
B. To reduce computational cost while maintaining performance
C. To enhance vocabulary size
D. To remove unnecessary parameters

Answer: B

Which technique helps mitigate hallucinations in LLMs?
A. Using adversarial training exclusively
B. Prompt engineering and retrieval-augmented generation
C. Reducing dataset size
D. Increasing response randomness

Answer: B

Part B [4 x 2 points = 8] : Multiple Choice Questions (MCQs)

How does Few-Shot Learning differ from Zero-Shot Learning?
A. Few-Shot Learning requires examples, while Zero-Shot Learning does not
B. Few-Shot Learning is only applicable to image models
C. Zero-Shot Learning is more accurate
D. Few-Shot Learning needs a separate dataset

Answer: A

What is a common optimization challenge in LLM training?
A. Handling small dataset sizes
B. Managing extensive memory requirements
C. Reducing the number of transformer layers
D. Eliminating positional embeddings

Answer: B

Which technique allows LLMs to generate structured output?
A. Random sampling
B. Reinforcement learning from human feedback (RLHF)
C. Beam search decoding
D. Manual rule-based programming

Answer: C

How does LoRA fine-tuning improve LLM adaptation?
A. By freezing most model weights and tuning low-rank matrices
B. By replacing attention mechanisms with recurrent layers
C. By training a model from scratch
D. By requiring extensive computational resources

Answer: A

Part C [2 x 6 points = 12]: MCQs with Explanation

What makes chain-of-thought (CoT) prompting effective?
A. It breaks problems into step-by-step reasoning
B. It removes ambiguity from text inputs
C. It requires massive labeled datasets
D. It improves performance only for image generation

Answer: A
Explanation: CoT allows LLMs to improve reasoning capabilities by structuring responses as logical steps.

Why is Reinforcement Learning from Human Feedback (RLHF) used in fine-tuning LLMs?
A. To align model outputs with human preferences
B. To replace all pre-training data
C. To eliminate data bias completely
D. To increase tokenization efficiency

Answer: A
Explanation: RLHF helps ensure that model outputs are aligned with human feedback and ethical standards.
